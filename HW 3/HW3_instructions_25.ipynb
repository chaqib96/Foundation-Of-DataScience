{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will preprocess the dataset and perform some basic regression and classification tasks. The learning outcome of this part is to know how one can pre-process a real-world dataset and perform a supervised learning task, and to understand some of the fundamental mechanisms behind these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Pass/Fail.\n",
    "\n",
    "To Pass this HW you need to provide a complete and correct solution, passing all the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE: \n",
    "\n",
    "Data pre-processing, regression task and classification task\n",
    "\n",
    "1. Reading the files\n",
    "2. Missing Values\n",
    "3. Imputing categorical variables\n",
    "4. Imputing numerical variables\n",
    "5. Classification with Decision Tree, single split\n",
    "6. Classification with Decision Tree, Cross validation\n",
    "7. Interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important instructions:\n",
    "\n",
    "Each function you make will be considered during the grading, so it is important to strictly follow input and output instructions stated in the skeleton code.\n",
    "\n",
    "You must not change the names of the functions, since, if you do, the tests will fail.\n",
    "\n",
    "Since this Homework is, in part, focused on having you implement creative solutions to impute missing data, if at any point of the homework you will use functions like fillna(), SimpleImputer(), IterativeImputer(), or packages like fancyimpute, missingpy, or similar, you will fail a test designed to spot these packages. Please, try to avoid circumventing this rule, since een if you manage to pass the homework, a similar task might be in the exam, and there you would be spotted for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Scenario: Cleaning and Preparing Heart Disease Data\n",
    "\n",
    "You have recently joined the **Data Science and Analytics Unit** at the *Global Health Institute (GHI)*, a non-profit organization focused on improving cardiovascular disease diagnosis through data-driven research.  \n",
    "\n",
    "A junior data analyst from your team, **Franco**, sends you a message:\n",
    "\n",
    "> “Hey, welcome to the team! We’re preparing a predictive model to help doctors identify patients at risk of heart disease using clinical data from several hospitals.  \n",
    ">   \n",
    "> We have two related datasets:\n",
    "> - **Cleveland dataset** → this will be used for **training and validation**\n",
    "> - **Hungary dataset** → this will serve as our **independent test set**\n",
    ">\n",
    "> Unfortunately, it looks like something went wrong during the data collection process: some values appear to have been **corrupted or lost**. Before we can train any classification model, we need to **inspect and clean the data**, handle **missing or inconsistent values**, and make sure it’s ready for modeling. I'm completely lost and I have a lot of other work, can you please help me with the cleaning and with creating some baselines classification models?”\n",
    "\n",
    "Your task is to **analyze and clean the datasets** before **building a classifier** to predict whether a patient has heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the libraries that you will need throughout the assignment\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from HW import *\n",
    "\n",
    "RSEED = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.* Reading the files\n",
    "\n",
    "### `Task: Read the datasets from the 'datasets' folder. Use the files called cleveland.csv and hungary.csv that you have downloaded in this archive.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Dataset — Column Descriptions\n",
    "\n",
    "Someone has changed the names of some columns in the dataset, so make sure to use this description and refer to it for the \"allowed\" values.\n",
    "\n",
    "Common sense is useful when evaluating some of the features: for example, in this dataset there is no column called weight, but, if there was one, since we are talking about humans and not ethereal beings, if a patient had a value of 0 in the weight column, this value could be due to a typo, or corrupted, and would need to be cleaned in some way.\n",
    "\n",
    "| **Column** | **Description** |\n",
    "|-------------|-----------------|\n",
    "| **Age** | Age of the patient (in years). This dataset only includes adult patients. |\n",
    "| **Sex** | Biological sex of the patient: `1 = male`, `0 = female`. |\n",
    "| **ChestPainType** | Type of chest pain experienced: <br>• `1` = typical angina <br>• `2` = atypical angina <br>• `3` = non-anginal pain <br>• `4` = asymptomatic. |\n",
    "| **RestBP** | Resting blood pressure (in mm Hg) measured on admission to the hospital. |\n",
    "| **Chol** | Serum cholesterol level (in mg/dl). |\n",
    "| **FBS** | Fasting blood sugar: `1` if fasting blood sugar > 120 mg/dl, otherwise `0`. |\n",
    "| **RestECG** | Resting electrocardiographic results: <br>• `0` = normal <br>• `1` = ST-T wave abnormality <br>• `2` = showing probable or definite left ventricular hypertrophy. |\n",
    "| **MaxHR** | Maximum heart rate achieved during the exercise test. |\n",
    "| **ExAng** | Exercise-induced angina: `1` = yes, `0` = no. |\n",
    "| **Oldpeak** | ST depression induced by exercise relative to rest (a measure of exercise-induced ischemia). |\n",
    "| **Slope** | Slope of the peak exercise ST segment: <br>• `1` = upsloping <br>• `2` = flat <br>• `3` = downsloping. |\n",
    "| **Ca** | Number of major vessels (0–3) colored by fluoroscopy (a measure of blood flow). |\n",
    "| **Thal** | Thalassemia test result: <br>• `3` = normal <br>• `6` = fixed defect <br>• `7` = reversible defect. |\n",
    "| **Num** | Diagnosis of heart disease (target variable): <br>`0` = no heart disease, `1–4` = presence of heart disease with increasing severity. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the folder 'datasets', read the files cleveland.csv and hungary.csv into the dataframes cleveland and test, respectively.\n",
    "\n",
    "cleveland = pd.DataFrame()  # change this\n",
    "test = pd.DataFrame()       # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can uncomment this to inspact the datasets\n",
    "# cleveland.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "# cleveland.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "# test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.* Missing values\n",
    "\n",
    "### `Task: use the function clean_data from the HW.py file to get a clean version of the cleveland and test dataframes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "cleveland_cleaned, missing_values_cleveland = pd.DataFrame(), {} # change this\n",
    "test_cleaned, missing_values_test = pd.DataFrame(), {} # change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3.* Imputing categorical variables\n",
    "\n",
    "At the beginning of this file you can find the names of the columns and a description of their contents.\n",
    "\n",
    "Determine which columns are categorical, and set their type to object.\n",
    "\n",
    "Determine which columns are numerical, and set their type accordingly.\n",
    "\n",
    "Do not include the target column in any of these lists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []        # change this\n",
    "numerical_columns = []      # change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Split the cleveland_cleaned dataframe in a train and a validation set, using train_test_split from sklearn. `\n",
    "\n",
    "The train set must be called train, the validation set must be called val. The size of the validation set must be 30% of the total size of the cleveland_cleaned dataframe. Use shuffle=True and stratify the split based on y_cleveland. Make sure that both train and val are dataframes, and that the columns have the correct names. Reset the indexes of all four the dataframes, using drop=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y, where X contains the features and y contains the target variable.\n",
    "X_cleveland = pd.DataFrame()  # change this\n",
    "y_cleveland = pd.DataFrame()  # change this\n",
    "\n",
    "X_test = pd.DataFrame()       # change this\n",
    "y_test = pd.DataFrame()       # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame() # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to see information about the split dataset, uncomment:\n",
    "# X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to see information about the split dataset, uncomment:\n",
    "# X_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the classification task easier, transform the target variable into a binary variable.\n",
    "# If the target variable is 0, it should remain 0. If the target variable is more than 0, it should be transformed into 1.\n",
    "y_train = pd.DataFrame()  # change this\n",
    "y_val = pd.DataFrame()    # change this\n",
    "y_test = pd.DataFrame()   # change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: use the impute_missing_categorical function from the HW.py file to impute the missing data from the categorical features in your dataframes. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "X_train_imputed_cat, X_val_imputed_cat, X_test_imputed_cat = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4.* Imputing numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: use the impute_missing_numeric function from the HW.py file to impute the missing data from the numeric features in your dataframes. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "X_train_imputed_num, X_val_imputed_num, X_test_imputed_num = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: use the merge_imputed function from the HW.py file to merge your imputed dataframes. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the train_imputed_cat and train_imputed_num datasets. Call the resulting dataset X_train_imputed.\n",
    "# Merge the val_imputed_cat and val_imputed_num datasets. Call the resulting dataset X_val_imputed.\n",
    "# Merge the test_imputed_cat and test_imputed_num datasets. Call the resulting dataset X_test_imputed.\n",
    "\n",
    "# Write your code here\n",
    "X_train_imputed = pd.DataFrame()\n",
    "X_val_imputed = pd.DataFrame()\n",
    "X_test_imputed = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5.* Classification, using a single split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Use the function train_and_evaluate_single_split to produce classification results for your test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to run the hyperparameter tuning with a single split:  3.4809112548828125e-05\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters for the tree should be:\n",
    "# criterion: ['gini', 'entropy']\n",
    "# max_depth: [3, 5, 10]\n",
    "# The hyperparameters for the logistic regression should be:\n",
    "# penalty: ['l1', 'l2']\n",
    "# C: [0.1, 10]\n",
    "# solver: ['liblinear']\n",
    "\n",
    "# For each combination of hyperparameters, train a classification pipeline using your function.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "hyperparameters_tree = {} # change this\n",
    "hyperparameters_logreg = {} # change this\n",
    "performance_df = pd.DataFrame(columns=['params', 'F1 scores'])\n",
    "\n",
    "# create a list from the grid of hyperparameters for each model, and create the models.\n",
    "\n",
    "start = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "for number in range(1, 11): # change this\n",
    "    # call your function here, then concat the results to performance_df\n",
    "    pass # remove this line\n",
    "\n",
    "end = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "print('Time elapsed to run the hyperparameter tuning with a single split: ', end - start) # DO NOT CHANGE/DELETE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and validation datasets. Call the resulting datasets X and y.\n",
    "X = pd.DataFrame()  # change this\n",
    "y = pd.DataFrame()  # change this\n",
    "\n",
    "# retrain the model with the best hyperparameters on the whole training dataset.\n",
    "# Remember to use the same preprocessing steps as before.\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *6.* Classification with Decision Tree using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Use the function train_and_evaluate_cross_validation to produce classification results for your test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to run the hyperparameter tuning with Cross Validation:  0.0020859241485595703\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the same hyperparameters from the previous task.\n",
    "# 2. Create a dataframe to store the performance of the model with cross-validation, containing the columns 'params' and 'Average F1 scores'\n",
    "# 3. You can reuse the parameter grids from the previous step.\n",
    "# 4. Run your function for each combination of hyperparameters, using 5-fold cross-validation.\n",
    "# 5. Concatenate the results to the dataframe created in step 2.\n",
    "\n",
    "\n",
    "\n",
    "X = [[5,6], [10,11], [15,16], [20,21], [25,26], [30,31], [35,36], [40,41], [45,46], [50,51]]    # Delete this line\n",
    "y = [0,1,0,1,0,1,0,1,0,1]                                                                       # Delete this line\n",
    "\n",
    "X = pd.DataFrame(X)                                                                             # Delete this line\n",
    "y = pd.DataFrame(y)                                                                             # Delete this line\n",
    "\n",
    "\n",
    "# DO NOT FORGET TO DELETE THE PREVIOUS LINES. They are only to make the empty assignment run without errors,\n",
    "# but they will destroy the data you need.\n",
    "\n",
    "performance_df_cv = pd.DataFrame(columns=['params', 'F1 scores'])\n",
    "\n",
    "start_CV = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "# call your function here, then concat the results to performance_df_cv\n",
    "\n",
    "end_CV = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "print('Time elapsed to run the hyperparameter tuning with Cross Validation: ', end_CV - start_CV) # DO NOT CHANGE/DELETE THIS LINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain the model with the best hyperparameters on the whole training dataset.\n",
    "# Remember to use the same preprocessing steps as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *7.* Interpretation of the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Which model performs the best? `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your explanation here. Delete this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: use the best model to produce predictions on the test set, then calculate the F1 score on the test set. What do you notice? `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` What is a possible explanation? `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
