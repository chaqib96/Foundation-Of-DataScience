{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c36820",
   "metadata": {},
   "source": [
    "# HW1: Data Preprocessing, KPI, and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f61f4b",
   "metadata": {},
   "source": [
    "## Introduction: Exploring the Drivers of Hotel Booking Cancellations\n",
    "\n",
    "Imagine you are a **data science consultant** who has just been hired by two hotels.  \n",
    "The managers are worried about the growing number of **cancellations**, and they want to understand what drives this behavior so they can make better business decisions.  \n",
    "\n",
    "Your job in this homework is to help the hotel managers uncover the **main reasons behind cancellations**.  \n",
    "You will approach this problem using the **same structured workflow** you practiced in the Lab (remember when you were part of the airline’s *Customer Experience & Insights* team 😄).  \n",
    "\n",
    "This time, instead of analyzing passenger satisfaction, you are analyzing hotel bookings. The process is the same:  \n",
    "\n",
    "- Load and clean the dataset,  \n",
    "- Explore it systematically,  \n",
    "- Compute KPIs and summaries,  \n",
    "- Apply PCA to simplify complexity,  \n",
    "- And extract insights.  \n",
    "\n",
    "At the end, you should be able to **report the key factors behind cancellations**.  \n",
    "While this is not a formal requirement in the submission, try to **write a short text as if you were delivering recommendations to your client**. The goal is to practice turning analysis into **clear, data-driven advice** that managers can act on.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8514d0",
   "metadata": {},
   "source": [
    "**This homework is your first hands-on warm-up. By the end, you will be able to:**  \n",
    "- Load a real dataset and perform a **light, deterministic clean**.  \n",
    "- Compute **basic KPIs** and **categorical summaries** without plotting.  \n",
    "- Build a **PCA-ready numeric matrix** (impute + standardize) and run **PCA**.  \n",
    "- Interpret **explained variance ratios** and **dominant features** for principal components.  \n",
    "- Submit clean, testable code that passes **VPL** auto-grading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707cbdb",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset & Columns: Quick Guide\n",
    "\n",
    "**Data Source & Coverage**  \n",
    "This homework uses the public **Hotel booking demand** dataset. It contains **booking-level records** from two hotels in Portugal - a **City Hotel** and a **Resort Hotel** - covering bookings around **2015–2017**.  \n",
    "A copy is available on Kaggle: [Hotel booking demand (Mostipak)](https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand).\n",
    "\n",
    "> First time using Kaggle?\n",
    ">Use the link above to read about the dataset, explore the documentation, and get familiar with the platform.\n",
    "\n",
    "\n",
    "- **File:** `hotel_bookings.csv`  \n",
    "- **Size:** **119390 rows × 32 columns**  \n",
    "- **Goal of this HW:** understand **cancellations** and prepare numeric features for **PCA**.\n",
    "\n",
    "**What’s inside:** \n",
    "This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. All personally identifying information has been removed from the data.\n",
    "\n",
    "- Booking details (e.g., **lead time**, **stay length**, **ADR** price).  \n",
    "- Channel/segment info (e.g., **market_segment**, **distribution_channel**, **deposit_type**).  \n",
    "- Guest counts (**adults**, **children**, **babies**) and a flag for **repeated guests**.  \n",
    "- Dates (arrival year/month/day, and **reservation_status_date** for final status).  \n",
    "- The dataset is anonymized.\n",
    "\n",
    "**Target / Outcome**  \n",
    "- **`is_canceled`** — 0 (not canceled) or 1 (canceled).\n",
    "\n",
    "**Key categoricals used in EDA**\n",
    "- **`hotel`**\n",
    "- **`deposit_type`**\n",
    "- **`market_segment`**\n",
    "- **`distribution_channel`**\n",
    "- **`customer_type`**\n",
    "\n",
    "**Numeric features used for PCA (14)**\n",
    "- **`lead_time`**\n",
    "- **`stays_in_weekend_nights`**\n",
    "- **`stays_in_week_nights`**\n",
    "- **`adults`**\n",
    "- **`children`**\n",
    "- **`babies`**\n",
    "- **`previous_cancellations`**\n",
    "- **`previous_bookings_not_canceled`**\n",
    "- **`booking_changes`**\n",
    "- **`days_in_waiting_list`**\n",
    "- **`adr`**\n",
    "- **`required_car_parking_spaces`**\n",
    "- **`total_of_special_requests`**\n",
    "- **`total_guests`** *(we will create this: adults + children + babies)*\n",
    "\n",
    "> Tip: Don’t rename columns. VPL expects the standard names in your functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218710ad",
   "metadata": {},
   "source": [
    "## Before You Start  \n",
    "\n",
    "We will follow a **similar structured workflow** as in the lab. Since homeworks are automatically graded with the **VPL system**, and VPL cannot evaluate plots, we will check **numeric KPIs** instead of graphs.  \n",
    "\n",
    "➡️ **Recommendation:** Create your own plots locally while working through the notebook. This extra practice will help you explore the data more deeply and strengthen your intuition.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd64be",
   "metadata": {},
   "source": [
    "## Task 1: Load & Light Clean \n",
    "---\n",
    "\n",
    "**What you’ll build:** a function `load_and_clean(csv_path)` that returns a **cleaned** `pandas.DataFrame` ready for analysis.  \n",
    "You will **not** plot anything here. Keep the cleaning **deterministic** so everyone gets the same result.\n",
    "\n",
    "In short: These steps make sure dates are real dates, numbers are real numbers, bookings make sense (no zero guests), and errors are flagged as missing values instead of breaking the dataset.\n",
    "\n",
    "### Required steps (follow in this order)\n",
    "1) **Convert reservation date column into proper date format**  \n",
    "   - Convert the column `reservation_status_date` from text into a proper date format so it can be used in time-based analysis.\n",
    "   - If a value doesn’t look like a valid date, mark it as NaT (Not a Time), instead of crashing the process.\n",
    "\n",
    "2) **Convert numeric columns** (convert strings to numbers; invalids → `NaN`)  \n",
    "   - Columns to convert (if present):  \n",
    "     `is_canceled, lead_time, stays_in_weekend_nights, stays_in_week_nights, adults, children, babies,`  \n",
    "     `previous_cancellations, previous_bookings_not_canceled, booking_changes, days_in_waiting_list, adr,`  \n",
    "     `required_car_parking_spaces, total_of_special_requests, arrival_date_year, arrival_date_day_of_month`  \n",
    "   - Use `pd.to_numeric(col, errors=\"coerce\")` in a simple loop.\n",
    "   - If a value can’t be converted (e.g., text in a number column), replace it with `NaN`. \n",
    "\n",
    "3) **Create total guests**  \n",
    "   - Create new column: `total_guests = adults + children + babies` (row‑wise sum).  \n",
    "   - If any of these are `NaN`, the sum will be `NaN` — that’s fine for now.\n",
    "\n",
    "4) **Drop impossible rows**  \n",
    "   - Keep only rows where `total_guests > 0`. (Removes bookings with 0 guests.)\n",
    "\n",
    "5) **Handle negative prices**  \n",
    "   - If `adr < 0`, set it to `NaN` (do **not** drop those rows).\n",
    "   - ADR represents the average revenue earned per occupied room per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you changed hw1.py, reload the module so Jupyter sees your edits\n",
    "import numpy as np\n",
    "import importlib\n",
    "import hw1 as hw\n",
    "importlib.reload(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6df188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Task 1\n",
    "import os\n",
    "print(os.getcwd())\n",
    "CSV_file = \"hotel_bookings.csv\"\n",
    "df = hw.load_and_clean(CSV_file)\n",
    "\n",
    "print(\"Loaded & cleaned\")\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Do not change this part!  \n",
    "# These assertions mirror the VPL auto-grading checks.  \n",
    "# Use them locally to confirm that your function works properly.\n",
    "\n",
    "assert df.shape == (119210, 33)\n",
    "assert \"total_guests\" in df and (df[\"total_guests\"] > 0).all()\n",
    "assert (df[\"adr\"] < 0).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2c3c3",
   "metadata": {},
   "source": [
    "## Task 2: Basic Numeric KPIs\n",
    "---\n",
    "\n",
    "**What you’ll build:** a function `numeric_kpis(df)` that returns a small **dictionary of key numbers** describing the cleaned dataset from **Task 1**.  \n",
    "No plots, no printing required, just compute and return the values.\n",
    "\n",
    "In short: You’ll get a quick “health check” of the dataset: how big it is, how often bookings are canceled, how prices look at the high end, and how long stays typically are.\n",
    "\n",
    "### Required steps\n",
    "\n",
    "1) **Use the cleaned DataFrame from Task 1**  \n",
    "   - Input to this function is the **already cleaned** `df` from `load_and_clean(...)`.\n",
    "\n",
    "2) **Compute these KPIs and return them in a dict:**  \n",
    "   - **`rows`** → number of rows (after cleaning).  \n",
    "       \n",
    "   - **`cols`** → number of columns.  \n",
    "     \n",
    "   - **`cancel_rate`** → average of `is_canceled`. (NaNs are ignored automatically)\n",
    "\n",
    "   - **`adr_p95`** → 95th percentile of `adr` (a robust “high price” marker).  \n",
    "    \n",
    "   - **`avg_stay_len`** → mean of **total nights** per booking, where  \n",
    "     `total_nights = stays_in_week_nights + stays_in_weekend_nights`.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94276ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload again to reset any changes\n",
    "importlib.reload(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = hw.numeric_kpis(df)\n",
    "display(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71115ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Do not change this part!  \n",
    "# These assertions mirror the VPL auto-grading checks.  \n",
    "# Use them locally to confirm that your function works properly.\n",
    "\n",
    "assert k[\"rows\"] == 119210 and k[\"cols\"] == 33\n",
    "assert abs(k[\"cancel_rate\"] - 0.370766) < 1e-3\n",
    "assert abs(k[\"adr_p95\"] - 193.5) < 1e-6\n",
    "assert abs(k[\"avg_stay_len\"] - 3.426248) < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16f54e",
   "metadata": {},
   "source": [
    "## Task 3: Categorical EDA (no plots)\n",
    "---\n",
    "\n",
    "**What you’ll build:** a function `categorical_cancel_stats(df)` that summarizes **cancellation rates by category** and identifies which **market segment** has the highest cancellation rate among sufficiently common categories.  \n",
    "No plots — just compute the numbers and return them.\n",
    "\n",
    "In short: You’ll measure how cancellations differ across **hotel type** and **deposit policy**, and find the highest-risk **market segment** (only considering segments with enough data).\n",
    "\n",
    "### Required steps\n",
    "\n",
    "1) **Use the cleaned DataFrame from Task 1**  \n",
    "   - Input to this function is the **already cleaned** `df` from `load_and_clean(...)`.\n",
    "\n",
    "2) **Cancellation by hotel type** → `hotel_rates`  \n",
    "   - Group by `hotel` and compute the **mean of `is_canceled`**.  \n",
    "   - Convert to a Python dictionary.  \n",
    "\n",
    "3) **Cancellation by deposit type** → `deposit_rates`  \n",
    "   - Group by `deposit_type` and compute the **mean of `is_canceled`**.  \n",
    "   - Convert to a dictionary.  \n",
    "\n",
    "4) **Top market segment (n ≥ 500)** → `top_segment_500`  \n",
    "   - Count rows per `market_segment` and **keep only categories with at least 500 rows**.  \n",
    "   - Filter `df` to those segments, group by `market_segment`, compute the **mean cancellation rate**, and sort descending.  \n",
    "   - Take the **first** one and return it as a **tuple**: `(segment_name, rate)` where `rate` is a float.  \n",
    "    \n",
    "5) **Return format (exact keys):**  \n",
    "   ```python\n",
    "   {\n",
    "     \"hotel_rates\": <dict>,        # e.g., {\"City Hotel\": 0.xxx, \"Resort Hotel\": 0.yyy}\n",
    "     \"deposit_rates\": <dict>,      # e.g., {\"Non Refund\": 0.xxx, \"No Deposit\": 0.yyy, \"Refundable\": 0.zzz}\n",
    "     \"top_segment_500\": (<name>, <rate_float>)\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload again to reset any changes\n",
    "importlib.reload(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = hw.categorical_cancel_stats(df)\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8df4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Do not change this part!  \n",
    "# These assertions mirror the VPL auto-grading checks.  \n",
    "# Use them locally to confirm that your function works properly.\n",
    "\n",
    "assert \"City Hotel\" in cat[\"hotel_rates\"] and \"Resort Hotel\" in cat[\"hotel_rates\"]\n",
    "assert cat[\"top_segment_500\"][0] == \"Groups\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0e19d",
   "metadata": {},
   "source": [
    "## Task 4: Build PCA-ready Matrix\n",
    "---\n",
    "\n",
    "**What you’ll build:** a function `build_pca_matrix(df)` that takes the **cleaned** DataFrame from Task 1 and returns a **NumPy array** of numeric features, ready for PCA.  \n",
    "No plots, no printing — just return the matrix.\n",
    "\n",
    "In short: You will **pick 14 numeric features**, **fill missing values with the median**, and **standardize** each feature to mean 0 and standard deviation 1.\n",
    "\n",
    "### Features to include (exactly these 14)\n",
    "- `lead_time`  \n",
    "- `stays_in_weekend_nights`  \n",
    "- `stays_in_week_nights`  \n",
    "- `adults`  \n",
    "- `children`  \n",
    "- `babies`  \n",
    "- `previous_cancellations`  \n",
    "- `previous_bookings_not_canceled`  \n",
    "- `booking_changes`  \n",
    "- `days_in_waiting_list`  \n",
    "- `adr`  \n",
    "- `required_car_parking_spaces`  \n",
    "- `total_of_special_requests`  \n",
    "- `total_guests`  *(created in Task 1)*\n",
    "\n",
    "> Do **not** include the target `is_canceled`. No categorical encoding is needed for this task.\n",
    "\n",
    "### Required steps\n",
    "1) **Select the columns** above, in that order.  \n",
    "   - You may store this list in a constant like `PCA_FEATURES` to keep it consistent.\n",
    "\n",
    "2) **Impute missing values with the median**  \n",
    "   - Use `sklearn.impute.SimpleImputer(strategy=\"median\")`.  \n",
    "   - Fit on your selected columns and transform to get a **fully numeric, NaN-free** array.\n",
    "\n",
    "3) **Standardize (z-score) each feature**  \n",
    "   - Use `sklearn.preprocessing.StandardScaler()`.  \n",
    "   - Fit on the imputed data, then transform so each column has mean ~0 and std ~1.\n",
    "\n",
    "4) **Return a NumPy array** (not a DataFrame)  \n",
    "   - Shape must be `(n_samples, 14)` and contain **no NaNs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1444363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload again to reset any changes\n",
    "importlib.reload(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hw.build_pca_matrix(df)\n",
    "X.shape, np.isnan(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Do not change this part!  \n",
    "# These assertions mirror the VPL auto-grading checks.  \n",
    "# Use them locally to confirm that your function works properly.\n",
    "\n",
    "assert X.shape == (len(df), 14) and not np.isnan(X).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25a078",
   "metadata": {},
   "source": [
    "## Task 5: Run PCA and Inspect Results\n",
    "---\n",
    "\n",
    "**What you’ll build:** a function `run_pca(X, n_components=3)` that fits **Principal Component Analysis** on the PCA-ready matrix from **Task 4** and returns two things:  \n",
    "1) the **explained variance ratios** for each component, and  \n",
    "2) the **component loadings** (the weights for each original feature).\n",
    "\n",
    "No plots are required — just compute and return the values.\n",
    "\n",
    "In short: You’ll compress the 14 standardized features into 3 principal components, then report how much variance each component explains and what linear combinations (loadings) define them.\n",
    "\n",
    "### Required steps\n",
    "\n",
    "1) **Use the matrix from Task 4**  \n",
    "   - Input is the NumPy array `X` produced by `build_pca_matrix(df)` (shape `(n_samples, 14)`, no NaNs).\n",
    "\n",
    "2) **Initialize PCA**  \n",
    "   - `from sklearn.decomposition import PCA`  \n",
    "   - `pca = PCA(n_components=3)`\n",
    "\n",
    "3) **Fit PCA on `X`**  \n",
    "   - `pca.fit(X)`\n",
    "\n",
    "4) **Return these two attributes**  \n",
    "   - `pca.explained_variance_ratio_`  → array of length **3**  \n",
    "   - `pca.components_`                → matrix of shape **(3, 14)**\n",
    "\n",
    "> Do **not** return the PCA object itself. Return the **tuple** `(explained_variance_ratio_, components_)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload again to reset any changes\n",
    "importlib.reload(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54671e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio, comps = hw.run_pca(X, n_components=3)\n",
    "print('variance ratios:', ratio, 'sum:', ratio.sum())\n",
    "features = hw.PCA_FEATURES\n",
    "for i, row in enumerate(comps):\n",
    "    j = np.argmax(np.abs(row))\n",
    "    print(f'PC{i+1} top feature: {features[j]} (loading={row[j]:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Do not change this part!  \n",
    "# These assertions mirror the VPL auto-grading checks.  \n",
    "# Use them locally to confirm that your function works properly.\n",
    "\n",
    "assert ratio.shape == (3,) and comps.shape == (3, 14)\n",
    "print(\"All local checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c5e77",
   "metadata": {},
   "source": [
    "\n",
    "## ✨ Optional Practice (Not Graded)\n",
    "---\n",
    "\n",
    "This part is **not graded**, but it is highly recommended as extra practice.  \n",
    "\n",
    "Congratulations! You’ve completed your assignment as a **data science consultant** for the hotels. You have:\n",
    "- Cleaned and structured messy booking data,  \n",
    "- Computed KPIs to get a clear picture of cancellations,  \n",
    "- Explored patterns across features,  \n",
    "- Applied PCA to simplify and highlight the most important drivers.  \n",
    "\n",
    "Now comes the most important part: **communication**.  \n",
    "The hotel managers don’t want code or plots! they want a clear story. As a consultant, your analysis is only as valuable as the clarity of your recommendations.  \n",
    "Take a few minutes to **summarize your findings in plain language** as if you were writing a short note to the hotel managers.  \n",
    "\n",
    "Your note should answer questions like:  \n",
    "- What are the **main reasons for cancellations**?  \n",
    "- Which factors appear less important?  \n",
    "- What practical advice can managers take away?  \n",
    "\n",
    "Keep it short: 5–7 sentences is enough. Managers don’t have time to read a long report with many pages! 😄 \n",
    "\n",
    "Think of this as practicing the skill of **turning data into decisions**, which is just as important as the analysis itself.  \n",
    "\n",
    "This closes the loop: from raw data → structured workflow → actionable business insight.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
