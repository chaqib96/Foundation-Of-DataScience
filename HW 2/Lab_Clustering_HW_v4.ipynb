{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Tuple, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_1.py\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the CSV dataset \"Cust_Segmentation.csv\" and return a pandas DataFrame.\n",
    "\n",
    "    TODO:\n",
    "      - Use pandas to read the CSV dataset \"Cust_Segmentation.csv\".\n",
    "      - Return the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement load_data()\")\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> Tuple[np.ndarray, List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Drop categorical columns, drop rows with NaN, keep numeric columns, and standardize.\n",
    "\n",
    "    Steps:\n",
    "      1) Drop columns: \"Customer Id\", \"Defaulted\", \"Address\"  (ignore if missing)\n",
    "      2) Keep only numeric columns\n",
    "      3) Drop rows containing NaNs\n",
    "      4) Standardize remaining features with StandardScaler (mean≈0, std≈1)\n",
    "      5) Return (X_scaled, feature_names, df_clean)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_scaled : np.ndarray        # shape (n_samples, n_features), standardized\n",
    "    feature_names : List[str]    # names of the numeric features used\n",
    "    df_clean : pd.DataFrame      # cleaned numeric DataFrame (pre-standardization)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The tests expect the banned columns to be removed and no NaNs in X.\n",
    "    \"\"\"\n",
    "    # TODO: implement:\n",
    "    #   drop_cols = [\"Customer Id\", \"Defaulted\", \"Address\"]\n",
    "    #   df_clean = ...\n",
    "    #   feature_names = ...\n",
    "    #   X_scaled = StandardScaler().fit_transform(...)\n",
    "    raise NotImplementedError(\"Implement preprocess(df)\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_2.py\n",
    "\n",
    "\n",
    "def elbow_inertia(X: np.ndarray, k_min: int = 1, k_max: int = 10, random_state: int = 42) -> List[float]:\n",
    "    \"\"\"\n",
    "    Fit KMeans for k in [k_min..k_max] and return the list of inertias.\n",
    "\n",
    "    TODO:\n",
    "      - Loop k from k_min to k_max (inclusive)\n",
    "      - Fit KMeans(n_clusters=k, random_state=random_state)\n",
    "      - Append km.inertia_ to a list\n",
    "      - Return the list of inertias (length should be k_max - k_min + 1)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement elbow_inertia(X, k_min, k_max, random_state)\")\n",
    "\n",
    "\n",
    "def identify_elbow_k() -> int:\n",
    "    \"\"\"\n",
    "    Identify the 'elbow' k from a list of inertias.\n",
    "    \n",
    "    TODO:\n",
    "      - Return the chosen k (int)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The tests will assert the number you return for this dataset.\n",
    "    \"\"\"\n",
    "    # TODO: implement heuristic and return k for this dataset\n",
    "    raise NotImplementedError(\"Implement identify_elbow_k()\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_3.py\n",
    "\n",
    "\n",
    "\n",
    "def kmeans_cluster(X: np.ndarray, n_clusters: int, random_state: int = 42) -> Tuple[np.ndarray, KMeans]:\n",
    "    \"\"\"\n",
    "    Fit KMeans and return (labels, fitted_model).\n",
    "\n",
    "    TODO:\n",
    "      - Initialize KMeans with given n_clusters/random_state\n",
    "      - labels = km.fit_predict(X)\n",
    "      - Return (labels, km)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement kmeans_cluster(X, n_clusters, random_state)\")\n",
    "\n",
    "\n",
    "def kmeans_add_labels_and_centroids(\n",
    "    df_clean: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    "    feature_names: List[str],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Add K-Means labels to df_clean and compute centroids per cluster.\n",
    "\n",
    "    Output:\n",
    "      - df_with_labels: df_clean + a new column 'cluster_kmeans'\n",
    "      - centroids_df:   DataFrame of per-cluster means for the columns in feature_names\n",
    "                        (rows indexed by cluster id, columns = feature_names)\n",
    "\n",
    "    TODO:\n",
    "      - Make a copy of df_clean\n",
    "      - Add column 'cluster_kmeans' with the given labels\n",
    "      - Group by 'cluster_kmeans' and compute mean over feature_names\n",
    "      - Return (df_with_labels, centroids_df)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement kmeans_add_labels_and_centroids(...)\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_4.py\n",
    "\n",
    "\n",
    "\n",
    "def _count_clusters(labels: np.ndarray) -> int:\n",
    "    \"\"\"Helper: number of clusters excluding noise (-1).\"\"\"\n",
    "    uniq = set(labels.tolist())\n",
    "    if -1 in uniq:\n",
    "        uniq.remove(-1)\n",
    "    return len(uniq)\n",
    "\n",
    "\n",
    "def dbscan_cluster_to_target_k(\n",
    "    X: np.ndarray,\n",
    "    target_k: int = 1,\n",
    ") -> Tuple[np.ndarray, DBSCAN]:\n",
    "    \"\"\"\n",
    "    Try a small grid of (eps, min_samples) values until you reach exactly target_k clusters\n",
    "    (excluding noise). Return (labels, fitted_model).\n",
    "\n",
    "    TODO:\n",
    "      - Define a small grid, e.g. eps in [0.5, 0.8, 1.0, 1.2, 1.5, 2.0], min_samples in [3, 5, 8]\n",
    "      - For each pair, fit DBSCAN and check number of clusters via _count_clusters\n",
    "      - If exactly target_k, return (labels, model). target_k should be equal to the elbow point in Task 2.\n",
    "      - If none match, return the best/last attempt\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement dbscan_cluster_to_target_k(X, target_k)\")\n",
    "\n",
    "\n",
    "def dbscan_add_labels(df_clean: pd.DataFrame, labels: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of df_clean with a new column 'cluster_dbscan' containing the labels.\n",
    "\n",
    "    TODO:\n",
    "      - Copy df_clean\n",
    "      - Add column 'cluster_dbscan' with the provided labels\n",
    "      - Return the new DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement dbscan_add_labels(df_clean, labels)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60329e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_5.py\n",
    "\n",
    "\n",
    "\n",
    "def compute_silhouettes(\n",
    "    X: np.ndarray,\n",
    "    km_labels: np.ndarray,\n",
    "    db_labels: np.ndarray,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute silhouette scores for K-Means and DBSCAN.\n",
    "\n",
    "    Notes:\n",
    "      - K-Means: use all samples\n",
    "      - DBSCAN: ignore noise points (label == -1). Only compute if >=2 clusters remain.\n",
    "\n",
    "    TODO:\n",
    "      - km_sil = silhouette_score(X, km_labels)\n",
    "      - For DBSCAN:\n",
    "          mask out noise; if at least 2 clusters remain and >=2 samples, compute score on masked data\n",
    "          else set db_sil = np.nan\n",
    "      - Return (km_sil, db_sil) as floats\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement compute_silhouettes(X, km_labels, db_labels)\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
